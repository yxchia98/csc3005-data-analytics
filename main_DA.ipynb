{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meter A\n",
    "Contains 87 instances of physical diagnostic parameters for an 8-path liquid USM. It\n",
    "has 37 attributes(features) and 2 classes or health states: \\\n",
    "(1) -- Flatness ratio \\\n",
    "(2) -- Symmetry \\\n",
    "(3) -- Crossflow \\\n",
    "(4)-(11) -- Flow velocity in each of the eight paths \\\n",
    "(12)-(19) -- Speed of sound in each of the eight paths \\\n",
    "(20) -- Average speed of sound in all eight paths \\\n",
    "(21)-(36) -- Gain at both ends of each of the eight paths \\\n",
    "(37) -- Class attribute or health state of meter: 1,2 \\\n",
    "Class '1' - Healthy \\\n",
    "Class '2' - Installation effects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "meter_a_df = pd.read_csv(\"./dataset/Meter A\", sep=\"\\t\", header=None)\n",
    "meter_a_headers = ['Flatness Ratio',\n",
    "                      'Symmetry',\n",
    "                      'Crossflow',\n",
    "                      'Flow Velocity 1',\n",
    "                      'Flow Velocity 2',\n",
    "                      'Flow Velocity 3',\n",
    "                      'Flow Velocity 4',\n",
    "                      'Flow Velocity 5',\n",
    "                      'Flow Velocity 6',\n",
    "                      'Flow Velocity 7',\n",
    "                      'Flow Velocity 8',\n",
    "                      'Speed of Sound 1',\n",
    "                      'Speed of Sound 2',\n",
    "                      'Speed of Sound 3',\n",
    "                      'Speed of Sound 4',\n",
    "                      'Speed of Sound 5',\n",
    "                      'Speed of Sound 6',\n",
    "                      'Speed of Sound 7',\n",
    "                      'Speed of Sound 8',\n",
    "                      'Average Speed of Sound',\n",
    "                      'Gain at both ends 1',\n",
    "                      'Gain at both ends 2',\n",
    "                      'Gain at both ends 3',\n",
    "                      'Gain at both ends 4',\n",
    "                      'Gain at both ends 5',\n",
    "                      'Gain at both ends 6',\n",
    "                      'Gain at both ends 7',\n",
    "                      'Gain at both ends 8',\n",
    "                      'Gain at both ends 9',\n",
    "                      'Gain at both ends 10',\n",
    "                      'Gain at both ends 11',\n",
    "                      'Gain at both ends 12',\n",
    "                      'Gain at both ends 13',\n",
    "                      'Gain at both ends 14',\n",
    "                      'Gain at both ends 15',\n",
    "                      'Gain at both ends 16',\n",
    "                      'Class Attribute/Health State']\n",
    "meter_a_df.columns = meter_a_headers\n",
    "meter_a_df.loc[meter_a_df['Class Attribute/Health State'] == 2, 'Class Attribute/Health State'] = 3         # change label 2 to 3, for common class label across all datasets\n",
    "meter_a_df_x = meter_a_df.drop(['Class Attribute/Health State'], axis=1, inplace=False)\n",
    "meter_a_df_y = meter_a_df[['Class Attribute/Health State']].copy()\n",
    "\n",
    "meter_b_df = pd.read_csv(\"./dataset/Meter B\", sep=\"\\t\", header=None)\n",
    "meter_b_headers = [\n",
    "    'Profile Factor',\n",
    "    'Symmetry',\n",
    "    'Crossflow',\n",
    "    'Swirl Angle',\n",
    "    'Flow Velocity 1',\n",
    "    'Flow Velocity 2',\n",
    "    'Flow Velocity 3',\n",
    "    'Flow Velocity 4',\n",
    "    'Average flow velocity in all four paths',\n",
    "    'Speed of Sound 1',\n",
    "    'Speed of Sound 2',\n",
    "    'Speed of Sound 3',\n",
    "    'Speed of Sound 4',\n",
    "    'Average Speed of Sound',\n",
    "    'Signal Strength 1',\n",
    "    'Signal Strength 2',\n",
    "    'Signal Strength 3',\n",
    "    'Signal Strength 4',\n",
    "    'Signal Strength 5',\n",
    "    'Signal Strength 6',\n",
    "    'Signal Strength 7',\n",
    "    'Signal Strength 8',\n",
    "    'Turbulence 1',\n",
    "    'Turbulence 2',\n",
    "    'Turbulence 3',\n",
    "    'Turbulence 4',\n",
    "    'Meter Performance',\n",
    "    'Signal Quality 1',\n",
    "    'Signal Quality 2',\n",
    "    'Signal Quality 3',\n",
    "    'Signal Quality 4',\n",
    "    'Signal Quality 5',\n",
    "    'Signal Quality 6',\n",
    "    'Signal Quality 7',\n",
    "    'Signal Quality 8',\n",
    "    'Gain at both ends 1',\n",
    "    'Gain at both ends 2',\n",
    "    'Gain at both ends 3',\n",
    "    'Gain at both ends 4',\n",
    "    'Gain at both ends 5',\n",
    "    'Gain at both ends 6',\n",
    "    'Gain at both ends 7',\n",
    "    'Gain at both ends 8',\n",
    "    'Transit Time 1',\n",
    "    'Transit Time 2',\n",
    "    'Transit Time 3',\n",
    "    'Transit Time 4',\n",
    "    'Transit Time 5',\n",
    "    'Transit Time 6',\n",
    "    'Transit Time 7',\n",
    "    'Transit Time 8',\n",
    "    'Class Attribute/Health State'\n",
    "]\n",
    "meter_b_df.columns = meter_b_headers\n",
    "meter_a_df.loc[meter_a_df['Class Attribute/Health State'] == 3, 'Class Attribute/Health State'] = 4         # change label 3 to 4, for common class label across all datasets\n",
    "meter_b_df_x = meter_b_df.drop(meter_b_df.columns[51], axis=1, inplace=False)\n",
    "meter_b_df_y = meter_b_df[['Class Attribute/Health State']].copy()\n",
    "\n",
    "meter_d_df = pd.read_csv(\"./dataset/Meter D\", sep=\"\\t\", header=None)\n",
    "meter_d_headers=[\n",
    "    'Profile Factor',\n",
    "    'Symmetry',\n",
    "    'Crossflow',\n",
    "    'Flow Velocity 1',\n",
    "    'Flow Velocity 2',\n",
    "    'Flow Velocity 3',\n",
    "    'Flow Velocity 4',\n",
    "    'Speed of Sound 1',\n",
    "    'Speed of Sound 2',\n",
    "    'Speed of Sound 3',\n",
    "    'Speed of Sound 4',\n",
    "    'Signal Strength 1',\n",
    "    'Signal Strength 2',\n",
    "    'Signal Strength 3',\n",
    "    'Signal Strength 4',\n",
    "    'Signal Strength 5',\n",
    "    'Signal Strength 6',\n",
    "    'Signal Strength 7',\n",
    "    'Signal Strength 8',\n",
    "    'Signal Quality 1',\n",
    "    'Signal Quality 2',\n",
    "    'Signal Quality 3',\n",
    "    'Signal Quality 4',\n",
    "    'Signal Quality 5',\n",
    "    'Signal Quality 6',\n",
    "    'Signal Quality 7',\n",
    "    'Signal Quality 8',\n",
    "    'Gain at both ends 1',\n",
    "    'Gain at both ends 2',\n",
    "    'Gain at both ends 3',\n",
    "    'Gain at both ends 4',\n",
    "    'Gain at both ends 5',\n",
    "    'Gain at both ends 6',\n",
    "    'Gain at both ends 7',\n",
    "    'Gain at both ends 8',\n",
    "    'Transit Time 1',\n",
    "    'Transit Time 2',\n",
    "    'Transit Time 3',\n",
    "    'Transit Time 4',\n",
    "    'Transit Time 5',\n",
    "    'Transit Time 6',\n",
    "    'Transit Time 7',\n",
    "    'Transit Time 8',\n",
    "    'Class Attribute/Health State'\n",
    "]\n",
    "meter_d_df.columns = meter_d_headers\n",
    "meter_d_df_x = meter_d_df.drop(meter_d_df.columns[36], axis=1, inplace=False)\n",
    "meter_d_df_y = meter_d_df[['Class Attribute/Health State']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Meter A class labels:', meter_a_df['Class Attribute/Health State'].unique(), 'Meter A size:', meter_a_df.shape)\n",
    "print('Meter B class labels:', meter_b_df['Class Attribute/Health State'].unique(), 'Meter B size:', meter_b_df.shape)\n",
    "print('Meter D class labels:', meter_d_df['Class Attribute/Health State'].unique(), 'Meter D size:', meter_d_df.shape)\n",
    "meter_a_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unionize datasets of Meter A, B, and D.\n",
    "Union of all datasets, and filling up missing columns with NaN values for further processing later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add different columns between Meter B and A, as well as D and A, into A\n",
    "diff = meter_b_df.columns.difference(meter_a_df.columns)\n",
    "print('Meter A missing columns from B:', diff.shape[0], diff.tolist())\n",
    "a = np.full(shape=(meter_a_df.shape[0], diff.shape[0]), fill_value=np.nan)\n",
    "a = pd.DataFrame(a, columns=diff.tolist())\n",
    "meter_a_full_df = pd.concat([meter_a_df, a], axis=1)\n",
    "diff = meter_d_df.columns.difference(meter_a_full_df.columns)\n",
    "print('New Meter A missing columns from D:', diff.shape[0], diff.tolist())\n",
    "a = np.full(shape=(meter_a_df.shape[0], diff.shape[0]), fill_value=np.nan)\n",
    "a = pd.DataFrame(a, columns=diff.tolist())\n",
    "meter_a_full_df = pd.concat([meter_a_full_df, a], axis=1)\n",
    "meter_a_full_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add different columns between Meter A and B, as well as D and B, into B\n",
    "diff = meter_a_df.columns.difference(meter_b_df.columns)\n",
    "print('Meter B missing columns from A:', diff.shape[0], diff.tolist())\n",
    "# check for \n",
    "a = np.full(shape=(meter_b_df.shape[0], diff.shape[0]), fill_value=np.nan)\n",
    "a = pd.DataFrame(a, columns=diff.tolist())\n",
    "meter_b_full_df = pd.concat([meter_b_df, a], axis=1)\n",
    "diff = meter_d_df.columns.difference(meter_b_full_df.columns)\n",
    "print('New Meter B missing columns from D:', diff.shape[0], diff.tolist())\n",
    "a = np.full(shape=(meter_b_df.shape[0], diff.shape[0]), fill_value=np.nan)\n",
    "a = pd.DataFrame(a, columns=diff.tolist())\n",
    "meter_b_full_df = pd.concat([meter_b_full_df, a], axis=1)\n",
    "meter_b_full_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add different columns between Meter B and D, as well as A and D, into D\n",
    "diff = meter_a_df.columns.difference(meter_d_df.columns)\n",
    "print('Meter D missing columns from A:', diff.shape[0], diff.tolist())\n",
    "# check for \n",
    "a = np.full(shape=(meter_d_df.shape[0], diff.shape[0]), fill_value=np.nan)\n",
    "a = pd.DataFrame(a, columns=diff.tolist())\n",
    "meter_d_full_df = pd.concat([meter_d_df, a], axis=1)\n",
    "diff = meter_b_df.columns.difference(meter_d_full_df.columns)\n",
    "print('New Meter D missing columns from B:', diff.shape[0], diff.tolist())\n",
    "a = np.full(shape=(meter_d_df.shape[0], diff.shape[0]), fill_value=np.nan)\n",
    "a = pd.DataFrame(a, columns=diff.tolist())\n",
    "meter_d_full_df = pd.concat([meter_d_full_df, a], axis=1)\n",
    "meter_d_full_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification of common features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = np.intersect1d(np.intersect1d(meter_a_full_df.columns, meter_b_full_df.columns), meter_d_full_df.columns)\n",
    "common.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Meter A, B, D datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([pd.concat([meter_a_full_df, meter_b_full_df], ignore_index=True), meter_d_full_df], ignore_index=True)\n",
    "print('Meter A rows and columns:', meter_a_full_df.shape)\n",
    "print('Meter B rows and columns:', meter_b_full_df.shape)\n",
    "print('Meter D rows and columns:', meter_d_full_df.shape)\n",
    "full_df_x = full_df.drop(['Class Attribute/Health State'], axis=1, inplace=False)\n",
    "full_df_y = full_df[['Class Attribute/Health State']].copy()\n",
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with the union of datasets Meter A, B, and D, we will have a dataset of 359 rows and 69 attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Missing Values using K Nearest Neighbours\n",
    "Given that the datasets are all retrieved from a similar medium of Liquefied Natural Gas, and all have similar foundational properties retrieved, we use K Nearest Neighbours to fill up missing data, referencing from data that have features similar to the current missing one. \\\n",
    "Subsequently, if there are still zero values in the data, we will be replacing it with averages for a more accurate representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = full_df_x.isnull().sum()\n",
    "\n",
    "print('Total number of missing values:', sum(missing_values.tolist()))\n",
    "print(missing_values)\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "impute_knn = KNNImputer(n_neighbors=2)\n",
    "full_df_x_replaced = pd.DataFrame(impute_knn.fit_transform(full_df_x), columns=full_df_x.columns)\n",
    "\n",
    "\n",
    "# replace zero values with average of column\n",
    "\n",
    "full_df_x_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = (full_df_x_replaced == 0).sum()\n",
    "print('Total number of zero values:', sum(count.tolist()))\n",
    "print(count)\n",
    "full_df_x_replaced=full_df_x_replaced.mask(full_df_x_replaced==0).fillna(full_df_x_replaced.mean())\n",
    "print('\\nAfter Replacement:')\n",
    "full_df_x_replaced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recalculate Averages\n",
    "As averages are precise numbers computed based on features of the current row, we can not simply replace averages. Therefore, these averages have to be recalculated.\\\n",
    "There are two attributes that takes in average, and these are:\\\n",
    "1. Average flow velocity in all paths\n",
    "2. Average speed of sound in all paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df_x_replaced.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_velocity = full_df_x_replaced[['Flow Velocity 1', 'Flow Velocity 2', 'Flow Velocity 3', 'Flow Velocity 4', 'Flow Velocity 5', 'Flow Velocity 6', 'Flow Velocity 7', 'Flow Velocity 8']]\n",
    "average_speed = full_df_x_replaced[['Speed of Sound 1', 'Speed of Sound 2', 'Speed of Sound 3', 'Speed of Sound 4', 'Speed of Sound 5', 'Speed of Sound 6', 'Speed of Sound 7', 'Speed of Sound 8']]\n",
    "full_df_x_replaced['Average flow velocity in all four paths'] = avg_velocity.mean(axis=1)\n",
    "full_df_x_replaced['Average Speed of Sound'] = average_speed.mean(axis=1)\n",
    "full_df_x_replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates\n",
    "As duplicates affects the overall accuracy of the model, and having them might have similar records in train and test sets, it is important to remove exact duplicates before pushing these data into data mining models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_x_replaced.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Correlation between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any relation within columns, if there is a direct relation, choose one and remove the other\\\n",
    "We first get an absolute value of correlation, as we are interested in eliminating both strongly negative and positive correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Get absolute correlation matrix\n",
    "full_corr_matrix_x = full_df_x_replaced.corr().abs()\n",
    "\n",
    "plt.figure(figsize=(40,30))\n",
    "\n",
    "# Create a custom diverging palette\n",
    "cmap = sns.diverging_palette(250, 15, s=75, l=40,\n",
    "                             n=9, center=\"light\", as_cmap=True)\n",
    "\n",
    "_ = sns.heatmap(full_corr_matrix_x, center=0, annot=True, \n",
    "                fmt='.2f', square=True, cmap=cmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the two halves are identical. \\\n",
    "To get a more focused and better view of the matrix, we take only the first half, as both halfs are symmetrical. \\\n",
    "Using the mask method, the other half is filled with NaN values, and prevents unecessary computation as these sides are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,30))\n",
    "\n",
    "# Create a mask\n",
    "mask = np.triu(np.ones_like(full_corr_matrix_x, dtype=bool))\n",
    "\n",
    "sns.heatmap(full_corr_matrix_x, mask=mask, center=0, annot=True,\n",
    "             fmt='.2f', square=True, cmap=cmap)\n",
    "\n",
    "reduced_full_corr_matrix_x = full_corr_matrix_x.mask(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a threshold to drop the columns that has strong correlations, and get the list of columns to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [c for c in reduced_full_corr_matrix_x.columns if any(reduced_full_corr_matrix_x[c] > 0.90)]\n",
    "print(len(to_drop), 'rows to be dropped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the selected columns, resulting a dataset with redundant features eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_reduced_x = full_df_x_replaced.drop(to_drop, axis=1)\n",
    "full_df_reduced_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the ranking of features once again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the dataset\n",
    "With the scale of data varying between features, we can normalize the dataset to make the data appear similar across all records and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Apply standardisation to data\n",
    "full_df_reduced_x_scaled = StandardScaler().fit_transform(full_df_reduced_x)\n",
    "print('Minimum and Maximum before scaling:', full_df_reduced_x.min().min(), full_df_reduced_x.max().max())\n",
    "print('Minimum and Maximum after scaling:', full_df_reduced_x_scaled.min().min(), full_df_reduced_x_scaled.max().max())\n",
    "print('Mean and Standard Deviation after scaling:', round(full_df_reduced_x_scaled.mean()), full_df_reduced_x_scaled.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine and rank Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(full_df_reduced_x_scaled, np.reshape(full_df_y.to_numpy(), -1))\n",
    "extraTrees_df = pd.DataFrame(full_df_reduced_x_scaled, columns=full_df_reduced_x.columns)\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=extraTrees_df.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')\n",
    "extraTrees_df = full_df_x_replaced[feat_importances.nlargest(10).index]\n",
    "plt.show()\n",
    "extraTrees_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_x = full_df_reduced_x_scaled\n",
    "full_y = full_df_y.to_numpy()\n",
    "full_y = np.reshape(full_y, -1)\n",
    "print(full_x.shape)\n",
    "print(full_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_all = PCA()\n",
    "pca_all.fit(full_x)\n",
    "tot = sum(pca_all.explained_variance_)\n",
    "# var_exp = [(i / tot) for i in sorted(pca_all.explained_variance_, reverse=True)]\n",
    "var_exp = pca_all.explained_variance_ratio_\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(var_exp)\n",
    "print(len(var_exp))\n",
    "print('Captured eigen varience energy for PC1 to PC7:', sum(var_exp[:7]))\n",
    "# plot explained variances\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,8))\n",
    "ax1.bar(range(1, full_x.shape[1]+1), var_exp, alpha=0.5,\n",
    "        align='center', label='individual explained variance')\n",
    "ax1.step(range(1,full_x.shape[1]+1), cum_var_exp, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "ax1.set_ylabel('Explained variance ratio')\n",
    "ax1.set_xlabel('Principal component index')\n",
    "ax1.set_title('PCA Analysis for Merged Dataset')\n",
    "ax1.legend(loc='best')\n",
    "ax2.plot(np.cumsum(pca_all.explained_variance_ratio_))\n",
    "ax2.set_xlabel('number of components')\n",
    "ax2.set_ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graphs, we see that PC1 to PC7 captures 82.7% of the variation of data, which is sufficient for representation. Therefore, 7 PCS are sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=7)\n",
    "pca.fit(full_x)\n",
    "full_x_pca=pca.transform(full_x)\n",
    "print(full_x_pca.shape) # 7 PC used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train-test split of the data, with 80% training and 20% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(full_x_pca, full_y, test_size=0.2, random_state=12)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "# Store accuracy in a dictionary for later analysis\n",
    "accuracy_store = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "reds = full_y == 1           \n",
    "greens = full_y == 2\n",
    "blues = full_y == 3\n",
    "yellows = full_y == 4\n",
    "sns.set(style = \"darkgrid\")\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "ax.scatter(xs=full_x_pca[reds, 0], ys=full_x_pca[reds, 1], zs=full_x_pca[reds, 2], c='red')\n",
    "ax.scatter(xs=full_x_pca[greens, 0], ys=full_x_pca[greens, 1], zs=full_x_pca[greens, 2], c='green')\n",
    "ax.scatter(xs=full_x_pca[blues, 0], ys=full_x_pca[blues, 1], zs=full_x_pca[blues, 2], c='blue')\n",
    "ax.scatter(xs=full_x_pca[yellows, 0], ys=full_x_pca[yellows, 1], zs=full_x_pca[yellows, 2], c='yellow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.set(style = \"darkgrid\")\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.scatter(x=full_x_pca[reds, 0], y=full_x_pca[reds, 1], c='red')\n",
    "ax.scatter(x=full_x_pca[greens, 0], y=full_x_pca[greens, 1],  c='green')\n",
    "ax.scatter(x=full_x_pca[blues, 0], y=full_x_pca[blues, 1], c='blue')\n",
    "ax.scatter(x=full_x_pca[yellows, 0], y=full_x_pca[yellows, 1], c='yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Fit regression model to the training set\n",
    "regr.fit(X_train, Y_train)\n",
    "Y_pred_test = regr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize and evaluate the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "sns.scatterplot(x=Y_test, y=Y_pred_test, color='red')\n",
    "plt.title('Comparing Measured and predicted values for test set')\n",
    "plt.xlabel('Measured values for y')\n",
    "plt.ylabel('Predicted values for y')\n",
    "plt.show()\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Root mean squared error = %.4f\" % np.sqrt(mean_squared_error(Y_test, Y_pred_test)))\n",
    "print('R-squared = %.4f' % r2_score(Y_test, Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Slope = ', regr.coef_[0])   #get the gradient/regression\n",
    "print('Intercept = ', regr.intercept_)### Step 4: Postprocessing\n",
    "sns.scatterplot(x=X_test[:,0], y=Y_test,  color='red')\n",
    "sns.lineplot(x=X_test[:,0], y=Y_pred_test, color='blue')\n",
    "titlestr = 'Predicted Function: y = %.5fx1 + %.5fx2 + %.5fx3 + %.5fx4 + %.5fx5 + %.5fx6 + %.5fx7 + %.5f' % (regr.coef_[0], regr.coef_[1], regr.coef_[2], regr.coef_[3], regr.coef_[4], regr.coef_[5], regr.coef_[6], regr.intercept_)\n",
    "plt.title(titlestr)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Linear Regression is not that of a good regressor for our dataset, as the plotted line does not really predict that well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "maxdepths = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "\n",
    "trainAccuracy = np.zeros(len(maxdepths))\n",
    "testAccuracy = np.zeros(len(maxdepths))\n",
    "\n",
    "index = 0\n",
    "for depth in maxdepths:\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_predTrain = clf.predict(X_train)\n",
    "    Y_predTest = clf.predict(X_test)\n",
    "    trainAccuracy[index] = accuracy_score(Y_train, Y_predTrain)\n",
    "    testAccuracy[index] = accuracy_score(Y_test, Y_predTest)\n",
    "    index += 1\n",
    "print('Max Depth = 9 Training Accuracy:', trainAccuracy[8], '\\nMax Depth = 9 Testing Accuracy:', testAccuracy[8])   \n",
    "# Plot training and testing accuracies\n",
    "plt.plot(maxdepths,trainAccuracy,'ro-',maxdepths,testAccuracy,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('Max depth')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "accuracy_store['Decision Tree Classifier'] = testAccuracy[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the knee point for our test accuracy is at Max Depth = 9. Therefore, we take 9 as the optimal max depth for the decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "numNeighbors = [1,3,5,8,10,13,15,18,20,23,25,28,30,33,35,38,40,43,35,38,50]\n",
    "trainAcc = []\n",
    "testAcc = []\n",
    "\n",
    "for k in numNeighbors:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_predTrain = clf.predict(X_train)\n",
    "    Y_predTest = clf.predict(X_test)\n",
    "    trainAcc.append(accuracy_score(Y_train, Y_predTrain))\n",
    "    testAcc.append(accuracy_score(Y_test, Y_predTest))\n",
    "\n",
    "sns.lineplot(x=numNeighbors, y=trainAcc, marker='o')\n",
    "sns.lineplot(x=numNeighbors, y=testAcc, marker='o')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "print('K=13 Training accuracy:', trainAcc[5], '\\nK=13 Testing Accuracy', testAcc[5])\n",
    "\n",
    "accuracy_store['KNN'] = testAccuracy[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the knee point for the above graph is K=13. Therefore, let the optimal K value for K Nearest Neighbours be 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model=GaussianNB()\n",
    "nb_model.fit(X_train, Y_train)\n",
    "Y_predTrain = nb_model.predict(X_train)\n",
    "Y_predTest = nb_model.predict(X_test)\n",
    "trainAcc=accuracy_score(Y_train, Y_predTrain)\n",
    "testAcc=accuracy_score(Y_test, Y_predTest)\n",
    "print(\"Training Accuracy:\",trainAcc)\n",
    "print(\"Testing Accuracy:\",testAcc)\n",
    "\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=Y_predTest, s=100, cmap='RdBu')\n",
    "\n",
    "accuracy_store['Naive Bayes'] = testAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "LRtrainAcc = []\n",
    "LRtestAcc = []\n",
    "\n",
    "for param in C:\n",
    "    lr_model = LogisticRegression(C=param, solver='lbfgs')\n",
    "    lr_model.fit(X_train, Y_train)\n",
    "    Y_predTrain = lr_model.predict(X_train)\n",
    "    Y_predTest = lr_model.predict(X_test)\n",
    "    LRtrainAcc.append(accuracy_score(Y_train, Y_predTrain))\n",
    "    LRtestAcc.append(accuracy_score(Y_test, Y_predTest))\n",
    "sns.lineplot(x=C, y=LRtrainAcc, marker=\"o\")\n",
    "sns.lineplot(x=C, y=LRtestAcc, marker=\"o\")\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Accuracy')\n",
    "print('C=1.5 Training Accuracy:', LRtrainAcc[6], '\\nC=1.5 Testing Accuracy:', LRtestAcc[6])\n",
    "\n",
    "accuracy_store['Logistic Regression'] = LRtestAcc[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the knee point for Logistic Regression is at a inverse regularization strength of 1.5. Therefore, let the optimal parameters be C=1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 6, 7, 8, 9, 10]\n",
    "SVMtrainAcc = []\n",
    "SVMtestAcc = []\n",
    "\n",
    "for param in C:\n",
    "    svm_model = SVC(C=param,kernel='linear')\n",
    "    svm_model.fit(X_train, Y_train)\n",
    "    Y_predTrain = svm_model.predict(X_train)\n",
    "    Y_predTest = svm_model.predict(X_test)\n",
    "    SVMtrainAcc.append(accuracy_score(Y_train, Y_predTrain))\n",
    "    SVMtestAcc.append(accuracy_score(Y_test, Y_predTest))\n",
    "\n",
    "sns.lineplot(x=C, y=SVMtrainAcc, marker=\"o\")\n",
    "sns.lineplot(x=C, y=SVMtestAcc, marker=\"o\")\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Accuracy')\n",
    "print('C=3 Training Accuracy:', SVMtrainAcc[9], '\\nC=3 Testing Accuracy:', SVMtestAcc[9])\n",
    "\n",
    "accuracy_store['SVM'] = SVMtestAcc[9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the knee point for SVM is at a inverse regularization strength of 3. Therefore, let the optimal parameters be C=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 6, 7, 8, 9, 10]\n",
    "SVMtrainAcc = []\n",
    "SVMtestAcc = []\n",
    "\n",
    "for param in C:\n",
    "    nonlinear_svm_model = SVC(C=param,kernel='rbf',gamma='auto')\n",
    "    nonlinear_svm_model.fit(X_train, Y_train)\n",
    "    Y_predTrain = nonlinear_svm_model.predict(X_train)\n",
    "    Y_predTest = nonlinear_svm_model.predict(X_test)\n",
    "    SVMtrainAcc.append(accuracy_score(Y_train, Y_predTrain))\n",
    "    SVMtestAcc.append(accuracy_score(Y_test, Y_predTest))\n",
    "\n",
    "sns.lineplot(x=C, y=SVMtrainAcc, marker=\"o\")\n",
    "sns.lineplot(x=C, y=SVMtestAcc, marker=\"o\")\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Accuracy')\n",
    "print('C=7 Training Accuracy:', SVMtrainAcc[15], '\\nC=7 Testing Accuracy:', SVMtestAcc[15])\n",
    "\n",
    "accuracy_store['NLSVM'] = SVMtestAcc[15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the training and test accuracy starts to converge at a regularization parameter of 15. Therefore, we take the 15 as the optimal regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layer_sizes = [(30,30,30), (40,40,40), (50,50,50), (60,60,60), (70,70,70), (80,80,80)]\n",
    "trainAcc = []\n",
    "testAcc = []\n",
    "\n",
    "for k in hidden_layer_sizes:\n",
    "    nn_model = MLPClassifier(solver='adam',hidden_layer_sizes=k, learning_rate='adaptive',random_state=12,max_iter=1000)\n",
    "    nn_model.fit(X_train, Y_train)\n",
    "    Y_predTrain = nn_model.predict(X_train)\n",
    "    Y_predTest = nn_model.predict(X_test)\n",
    "    trainAcc.append(accuracy_score(Y_train, Y_predTrain))\n",
    "    testAcc.append(accuracy_score(Y_test, Y_predTest))\n",
    "hidden_layer_size=[x[0] for x in hidden_layer_sizes]\n",
    "plt.plot(hidden_layer_size, trainAcc, 'ro-', hidden_layer_size, testAcc,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('Number of hidden layer')\n",
    "plt.ylabel('Accuracy')\n",
    "print('(40,40,40) Training Accuracy:', trainAcc[3], '\\n(40,40,40) Testing Accuracy:', testAcc[1])\n",
    "\n",
    "accuracy_store['Neural Network'] = testAcc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the knee point where the model performs well both in training and testing accuracy is having (40, 40, 40) hidden layers. Therefore, we take (40, 40, 40) as the optimal hidden layer sizes as increasing the sizes does not give any significant improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Using Statistical Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_full_x_pca = pd.DataFrame(full_x_pca)\n",
    "\n",
    "N,d = df_full_x_pca.shape\n",
    "delta = pd.DataFrame(100*np.divide(df_full_x_pca.iloc[1:,:].values-df_full_x_pca.iloc[:N-1,:].values, df_full_x_pca.iloc[:N-1,:].values),\n",
    "                    columns=df_full_x_pca.columns, index=df_full_x_pca.iloc[1:].index)\n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9,6) # The default value of the figsize parameter is [6.4, 4.8]\n",
    "\n",
    "fig = plt.figure(figsize=(9,6)).gca(projection='3d')\n",
    "fig.scatter(delta.iloc[:, 0],delta.iloc[:, 1],delta.iloc[:, 2])\n",
    "fig.set_xlabel('PC1')\n",
    "fig.set_ylabel('PC2')\n",
    "fig.set_zlabel('PC3')\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanValue = delta.mean()\n",
    "covValue = delta.cov()\n",
    "print('meanValue', '\\n', meanValue)\n",
    "print('covValue',  '\\n', covValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "X = delta.values\n",
    "S = covValue.values\n",
    "for i in range(7):\n",
    "    X[:,i] = X[:,i] - meanValue[i]\n",
    "\n",
    "def mahalanobis(row):\n",
    "    # matmul: Matrix product of two arrays\n",
    "    # dot: Dot product of two arrays\n",
    "    return np.matmul(row,S).dot(row)   \n",
    "    \n",
    "anomaly_score = np.apply_along_axis(mahalanobis, axis=1, arr=X) # Apply a function to 1-D slices along the given axis.\n",
    "\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# We use PC4 in our visualization to help identify the outliers in a 3D space.\n",
    "p = ax.scatter(delta.iloc[:, 0],delta.iloc[:, 1],delta.iloc[:, 3],c=anomaly_score,cmap='jet')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC4')\n",
    "fig.colorbar(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom = pd.DataFrame(anomaly_score, index=delta.index, columns=['Anomaly score'])\n",
    "result = pd.concat((delta,anom), axis=1)\n",
    "result.nlargest(5,'Anomaly score') # Return the first n rows with the largest values in columns, in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n",
    "\n",
    "ts1 = delta[256:263]\n",
    "_ = ts1.plot.line(ax=ax1)\n",
    "ax1.set_ylabel('Percent Change')\n",
    "\n",
    "ts2 = delta[235:242]\n",
    "_ = ts2.plot.line(ax=ax2)\n",
    "ax2.set_ylabel('Percent Change')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "knn = 4\n",
    "nbrs = NearestNeighbors(n_neighbors=knn, metric=distance.euclidean).fit(delta.values)\n",
    "distances, indices = nbrs.kneighbors(delta.values)\n",
    "\n",
    "anomaly_score = distances[:,knn-1]\n",
    "\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "p = ax.scatter(delta.iloc[:, 0],delta.iloc[:, 1],delta.iloc[:, 3],c=anomaly_score,cmap='jet')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC4')\n",
    "fig.colorbar(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom = pd.DataFrame(anomaly_score, index=delta.index, columns=['Anomaly score'])\n",
    "result = pd.concat((delta,anom), axis=1)\n",
    "result.nlargest(5,'Anomaly score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ts = delta[256:263]\n",
    "ts.plot.line(ax=ax)\n",
    "ax.set_ylabel('Percent Change')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of Decision Boundary on various classifiers\n",
    "To better analyse the decision mechanisms of the applied data mining algorithms, we can plot decision boundary visualisations based on these algorithms with our found optimal parameters:\n",
    "1. Multilinear Linear Regression\n",
    "2. Decision Tree (Max Depth = 9)\n",
    "3. K Nearest Neighbours (K=13)\n",
    "4. Naive Bayes\n",
    "5. Logistic Regression (C=1.5)\n",
    "6. Support Vector Machine (C=3)\n",
    "7. Non-linear Support Vector Machine (C=15)\n",
    "8. Neural Network 40-40-40 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA with 2 components to better visualize the decision boundary\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(full_x)\n",
    "full_x_pca_2=pca.transform(full_x)\n",
    "\n",
    "x_min, x_max = full_x_pca_2[:, 0].min() - 1, full_x_pca_2[:, 0].max() + 1\n",
    "y_min, y_max = full_x_pca_2[:, 1].min() - 1, full_x_pca_2[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "X_train_2, X_test_2, Y_train_2, Y_test_2 = train_test_split(full_x_pca_2, full_y, test_size=0.2, random_state=12)\n",
    "\n",
    "\n",
    "#  Optimal Multivariate Linear Regression\n",
    "regr_clf = linear_model.LinearRegression()\n",
    "regr_clf = regr_clf.fit(X_train_2, Y_train_2)\n",
    "\n",
    "#  Optimal Decision Tree Classifier (Max Depth = 7)\n",
    "tree_clf = tree.DecisionTreeClassifier(max_depth=9)\n",
    "tree_clf = tree_clf.fit(X_train_2, Y_train_2)\n",
    "\n",
    "# Optimal K Nearest Neighbours (K=5)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=13, metric='minkowski', p=2)\n",
    "knn_clf = knn_clf.fit(X_train_2, Y_train_2)\n",
    "\n",
    "# Optimal Naive Bayes\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf = nb_clf.fit(X_train_2, Y_train_2)\n",
    "\n",
    "# Optimal Logistic Regression (C=1)\n",
    "lr_clf = LogisticRegression(C=1.5, solver='lbfgs')\n",
    "lr_clf = lr_clf.fit(X_train_2, Y_train_2)\n",
    "\n",
    "# Optimal Support Vector Machine (C=1)\n",
    "l_svm_clf = SVC(C=3, kernel='linear')\n",
    "l_svm_clf = l_svm_clf.fit(X_train_2, Y_train_2)\n",
    "\n",
    "# Non-linear Support Vector Machine (C=3)\n",
    "nl_svm_clf = SVC(C=15, kernel='rbf',gamma='auto')\n",
    "nl_svm_clf = nl_svm_clf.fit(X_train_2, Y_train_2)\n",
    "\n",
    "# Neural Network (40,40,40) layers\n",
    "nn_clf = MLPClassifier(solver='adam',hidden_layer_sizes=(40,40,40), learning_rate='adaptive',random_state=12,max_iter=1000)\n",
    "nn_clf = nn_clf.fit(X_train_2, Y_train_2)\n",
    "\n",
    "\n",
    "# Plot label positions\n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.set(style = \"darkgrid\")\n",
    "ax.set_title('Orginal classification of labels')\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.scatter(x=full_x_pca[reds, 0], y=full_x_pca[reds, 1], c='red')\n",
    "ax.scatter(x=full_x_pca[greens, 0], y=full_x_pca[greens, 1],  c='green')\n",
    "ax.scatter(x=full_x_pca[blues, 0], y=full_x_pca[blues, 1], c='blue')\n",
    "ax.scatter(x=full_x_pca[yellows, 0], y=full_x_pca[yellows, 1], c='yellow')\n",
    "\n",
    "# Plot decision boundary\n",
    "f, axarr = plt.subplots(4, 2, sharex='col', sharey='row', figsize=(15, 10))\n",
    "\n",
    "for idx, clf, tt in zip([[0, 0],[0,1],[1,0],[1,1],[2,0],[2,1],[3,0],[3,1]], [regr_clf,tree_clf,knn_clf,nb_clf,lr_clf,l_svm_clf,nl_svm_clf,nn_clf],\n",
    "['Linear Regression','Decision Tree (Max Depth = 9)','K Nearest Neighbours (K=13)','Naive Bayes','Logistic Regression (C=1.5)','Support Vector Machine (C=3)',\n",
    "'Non-linear Support Vector Machine (C=15)','Neural Network 40-40-40 layers']):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # Z = clf.predict(X_test)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z,alpha=0.4)\n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(range(len(accuracy_store)), list(accuracy_store.values()), align='edge', width=0.3)\n",
    "plt.xticks(range(len(accuracy_store)),list(accuracy_store.keys()))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "357e20c070b07b7c8968be68eaf647c4a57c465e9760b010e66f6a7a49fd3daa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
